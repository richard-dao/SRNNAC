# -*- coding: utf-8 -*-
"""SPC - snnTorch Dataset Tutorial

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17W8Q_jhssnjWuPnGqK6RkWtVqHeS57gq

[<img src='https://github.com/jeshraghian/snntorch/blob/master/docs/_static/img/snntorch_alpha_w.png?raw=true' width="300">](https://github.com/jeshraghian/snntorch/)
[<img src='https://github.com/neuromorphs/tonic/blob/develop/docs/_static/tonic-logo-white.png?raw=true' width="200">](https://github.com/neuromorphs/tonic/)


# Training with Spiking Speech Commands snnTorch Tutorial

##### By: Richard Dao (rqdao@ucsc.edu), Annabel Truong (anptruon@ucsc.edu), Mira Prabhakar (miprabha@ucsc.edu)

<a href="https://colab.research.google.com/drive/17W8Q_jhssnjWuPnGqK6RkWtVqHeS57gq#scrollTo=69E4SzHiFC28">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
</a>

For a comprehensive overview on how SNNs work, and what is going on under the hood, [then you might be interested in the snnTorch tutorial series available here.](https://snntorch.readthedocs.io/en/latest/tutorials/index.html)
The snnTorch tutorial series is based on the following paper. If you find these resources or code useful in your work, please consider citing the following source:


> <cite> [Jason K. Eshraghian, Max Ward, Emre Neftci, Xinxin Wang, Gregor Lenz, Girish Dwivedi, Mohammed Bennamoun, Doo Seok Jeong, and Wei D. Lu. "Training Spiking Neural Networks Using Lessons From Deep Learning". Proceedings of the IEEE, 111(9) September 2023.](https://ieeexplore.ieee.org/abstract/document/10242251) </cite>

This example tutorial will cover training spiking neural networks for an audio-based classication dataset.

First, install the snntorch library if it is not already installed on your machine.
"""

!pip install snntorch --quiet
!pip install tonic --quiet

"""And then we need to import some libraries to use for this project."""

# PyTorch
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from tonic import collation, datasets, DiskCachedDataset, transforms
import torchaudio

# snnTorch
import snntorch as snn
from snntorch import functional, utils

# Tonic
import tonic

# Other
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import random as rn
import os

from sklearn.model_selection import train_test_split
from IPython.display import HTML, display

"""# 1. The Spiking Speech Commands (SSC) Dataset"""

from google.colab import drive
drive.mount('/content/drive')

root = '/content/drive/My Drive/'
ext = 'snnTorch Research Project' # Use for developing

path = os.path.join(root, ext)

dataset = tonic.datasets.SSC(save_to=path, split='train')

events, target = dataset[0]
tonic.utils.plot_event_grid(events)

print(events.dtype)

sensor_size = datasets.SSC.sensor_size
time_step = 12000
downsample_factor = 1/4

toTensorTransform = transforms.Compose([
    transforms.Downsample(spatial_factor=downsample_factor),
    transforms.ToFrame(sensor_size=(700 // int(1 / downsample_factor), 1, 1), time_window=time_step)
])


train_dataset = tonic.datasets.SSC(save_to=path, split='train', transform=toTensorTransform)
validation_dataset = tonic.datasets.SSC(save_to=path, split='valid', transform=toTensorTransform)
test_dataset = tonic.datasets.SSC(save_to=path, split='test', transform=toTensorTransform)

batch_size = 128

train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collation.PadTensors(batch_first=False))
validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=True, collate_fn=collation.PadTensors(batch_first=False))
test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, collate_fn=collation.PadTensors(batch_first=False))

dataloaders = {
    'train':train_dataloader,
    'validation':validation_dataloader,
    'test':test_dataloader
}

# Defining the Network Architecture
device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")

inputs = int(700 * downsample_factor)
outputs = 35

beta = 0.95
lr = 0.001

model = nn.Sequential(
    # Define network architecture here
    nn.LSTM(input_size=inputs, hidden_size=512, batch_first=True),
    nn.Dropout(p=0.2),
    nn.Linear(in_features=512, out_features=512),
    snn.Leaky(beta=beta, init_hidden=True),
    nn.Linear(in_features=512, out_features=512),
    snn.Leaky(beta=beta, init_hidden=True),
    nn.Dropout(p=0.2),
    nn.Linear(in_features=512, out_features=35),
    snn.Leaky(beta=beta, init_hidden=True, output=True)
).to(device)

optimizer = optim.Adam(model.parameters(), lr = lr, betas=(0.9, 0.999))
criterion = functional.mse_count_loss(correct_rate=0.8, incorrect_rate=0.2)

def forward(net, data):
    spk_rec = []
    utils.reset(net)

    lstm_out, _ = net[0](data)

    x = net[1](lstm_out)
    # print("Data Shape: ", data.shape)
    for step in range(x.size(0)):
        fc1_out = net[2](x[step])
        spk1 = net[3](fc1_out)
        fc2_out = net[4](spk1)
        spk2 = net[5](fc2_out)
        fc3_out = net[6](spk2)
        spk3 = net[7](fc3_out)
        spk_rec.append(spk3)

    return torch.stack(spk_rec)

loss_history = {
    'train':[],
    'validation':[],
    'test':[]
}
accuracy_history = {
    'train':[],
    'validation':[],
    'test':[]
}

# Training Loop
num_epochs = 10
num_iters = 25

from tqdm.autonotebook import tqdm

with tqdm(range(num_epochs), unit='Epoch', desc='Training') as pbar:

    for _ in pbar:
        for phase in ['train', 'validation']:
            if phase == 'train':
                model.train()
            else:
                model.eval()
            epoch = 0
            for i, (events, labels) in enumerate(dataloaders[phase]):
                # print("Events Shape: ", events.shape)
                events = events.squeeze()
                # print("Events Shape After Squeeze: ", events.shape)
                events, labels = events.to(device), labels.to(device)
                # print("Labels Shape: ", labels.shape)

                optimizer.zero_grad()
                # Check the output to see what the values are
                with torch.set_grad_enabled(phase == 'train'):
                    spk_rec = forward(model, events)
                    # print('Classification Shape: ', spk_rec.shape)
                    loss = criterion(spk_rec, labels)

                    if phase == 'train':
                        loss.backward()
                        optimizer.step()

                    loss_history[phase].append(loss.item())
                    print(f"Epoch {epoch}, Iteration {i} \n{phase} loss: {loss.item():.2f}")
                    accuracy = functional.accuracy_rate(spk_rec, labels)
                    accuracy_history[phase].append(accuracy)

                    print(f"Accuracy: {accuracy * 100:.2f}%\n")

                    if i == num_iters:
                        break
            epoch += 1

train_fig = plt.figure(facecolor="w")
plt.plot(accuracy_history['train'])
plt.title("Train Set Accuracy")
plt.xlabel("Iteration")
plt.ylabel("Accuracy")
plt.show()

val_fig = plt.figure(facecolor="w")
plt.plot(accuracy_history['validation'])
plt.title("Validation Set Accuracy")
plt.xlabel("Iteration")
plt.ylabel("Accuracy")
plt.show()

model.eval()

with torch.no_grad():
    for events, labels in dataloaders['test']:
        events = events.squeeze()
        events, labels = events.to(device), labels.to(device)

        spk_rec = forward(model, events)
        loss = criterion(spk_rec, labels)

        loss_history['test'].append(loss.item())
        accuracy = functional.accuracy_rate(spk_rec, labels)
        accuracy_history['test'].append(accuracy)

test_fig = plt.figure(facecolor="w")
plt.plot(accuracy_history['test'])
plt.xlabel("Iteration")
plt.ylabel("Accuracy")
plt.show()

loss_comparison_fig = plt.figure(facecolor="w")
plt.plot(loss_history['train'], label='train')
plt.plot(loss_history['validation'], label='validation')
plt.plot(loss_history['test'], label='test')
plt.legend(loc='best')
plt.title(f"Train vs Validation vs Test - Loss Curves , LR = {lr} Batch Size = {batch_size} Epochs = {num_epochs}")
plt.xlabel("Iteration")
plt.ylabel("Loss")
plt.show()

accuracy_comparison_fig = plt.figure(facecolor="w")
plt.plot(accuracy_history['train'], label='train')
plt.plot(accuracy_history['validation'], label='validation')
plt.plot(accuracy_history['test'], label='test')
plt.legend(loc='best')
plt.title(f"Train vs Validation vs Test - Accuracy Curves , LR = {lr} Batch Size = {batch_size} Epochs = {num_epochs}")
plt.xlabel("Iteration")
plt.ylabel("Accuracy")
plt.show()